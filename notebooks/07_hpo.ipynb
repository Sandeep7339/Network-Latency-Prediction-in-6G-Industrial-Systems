{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecacac8",
   "metadata": {},
   "source": [
    "# 07 — Hyperparameter Optimisation (HPO)\n",
    "\n",
    "This notebook loads **pre-computed** HPO results (`reports/hpo_results.json`) and\n",
    "`models/best_params.json` generated by `python -m src.models.hpo`.\n",
    "\n",
    "**Pipeline recap**\n",
    "- Expanding-window **time-series CV** (4 folds, non-overlapping validation)\n",
    "- **XGBoost**: 12 randomised configs × 4 folds, with early stopping\n",
    "- **LSTM**: 6 randomised configs × 3 folds, 10 epochs max per trial\n",
    "\n",
    "We visualise:\n",
    "1. CV fold schematic\n",
    "2. XGBoost HPO param importance\n",
    "3. LSTM HPO learning-rate / hidden-dim landscape\n",
    "4. Best params summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "REPORTS = pathlib.Path('../reports')\n",
    "MODELS  = pathlib.Path('../models')\n",
    "FIGS    = pathlib.Path('../figures')\n",
    "FIGS.mkdir(exist_ok=True)\n",
    "\n",
    "with open(REPORTS / 'hpo_results.json') as f:\n",
    "    hpo = json.load(f)\n",
    "with open(MODELS / 'best_params.json') as f:\n",
    "    best = json.load(f)\n",
    "\n",
    "print(f\"HPO elapsed: {hpo['elapsed_seconds']:.0f}s\")\n",
    "print(f\"Keys: {list(hpo.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abd51a",
   "metadata": {},
   "source": [
    "## 1 — Time-Series CV Fold Schematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df082515",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 34000  # CV pool size\n",
    "n_splits = 4\n",
    "boundaries = [int(round(i * n_rows / (n_splits + 1))) for i in range(n_splits + 2)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "for fold_idx in range(n_splits):\n",
    "    train_end = boundaries[fold_idx + 1]\n",
    "    val_end   = boundaries[fold_idx + 2]\n",
    "    # train bar\n",
    "    ax.barh(fold_idx, train_end, left=0, height=0.6, color='#3b82f6', label='Train' if fold_idx == 0 else '')\n",
    "    # val bar\n",
    "    ax.barh(fold_idx, val_end - train_end, left=train_end, height=0.6, color='#f97316', label='Val' if fold_idx == 0 else '')\n",
    "\n",
    "ax.set_yticks(range(n_splits))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(n_splits)])\n",
    "ax.set_xlabel('Row index (sorted by timestamp_ns)')\n",
    "ax.set_title('Expanding-Window Time-Series CV')\n",
    "ax.legend(loc='lower right')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGS / 'hpo_cv_folds.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921ec9c",
   "metadata": {},
   "source": [
    "## 2 — XGBoost Regression HPO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = hpo['xgb_regression']\n",
    "df_xgb_reg = pd.DataFrame(xgb_reg['all_results'])\n",
    "\n",
    "# Expand params dict into columns\n",
    "params_df = pd.json_normalize(df_xgb_reg['params'])\n",
    "df_xgb_reg = pd.concat([params_df, df_xgb_reg[['mean_score', 'std_score']]], axis=1)\n",
    "df_xgb_reg = df_xgb_reg.sort_values('mean_score')\n",
    "\n",
    "print(f\"Metric: {xgb_reg['metric']}  |  Best: {xgb_reg['best_score']:.4f}\")\n",
    "print(f\"Best params: {xgb_reg['best_params']}\\n\")\n",
    "df_xgb_reg.head(12).style.background_gradient(subset=['mean_score'], cmap='RdYlGn_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "labels = [str(i+1) for i in range(len(df_xgb_reg))]\n",
    "ax.barh(labels, df_xgb_reg['mean_score'], xerr=df_xgb_reg['std_score'], color='#3b82f6', alpha=0.8)\n",
    "ax.set_xlabel(f\"CV Mean {xgb_reg['metric'].upper()}\")\n",
    "ax.set_ylabel('Config #')\n",
    "ax.set_title('XGBoost Regression — HPO Configs (sorted)')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGS / 'hpo_xgb_regression.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b1cbb",
   "metadata": {},
   "source": [
    "## 3 — XGBoost Classification HPO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = hpo['xgb_classification']\n",
    "df_xgb_clf = pd.DataFrame(xgb_clf['all_results'])\n",
    "params_df_clf = pd.json_normalize(df_xgb_clf['params'])\n",
    "df_xgb_clf = pd.concat([params_df_clf, df_xgb_clf[['mean_score', 'std_score']]], axis=1)\n",
    "df_xgb_clf = df_xgb_clf.sort_values('mean_score')\n",
    "\n",
    "print(f\"Metric: {xgb_clf['metric']}  |  Best: {xgb_clf['best_score']:.4f}\")\n",
    "print(f\"Best params: {xgb_clf['best_params']}\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "labels = [str(i+1) for i in range(len(df_xgb_clf))]\n",
    "ax.barh(labels, df_xgb_clf['mean_score'], xerr=df_xgb_clf['std_score'], color='#f97316', alpha=0.8)\n",
    "ax.set_xlabel(f\"CV Mean {xgb_clf['metric'].upper()}\")\n",
    "ax.set_ylabel('Config #')\n",
    "ax.set_title('XGBoost Classification — HPO Configs (sorted)')\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGS / 'hpo_xgb_classification.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbdc35",
   "metadata": {},
   "source": [
    "## 4 — LSTM HPO Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b656c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LSTM regression ──\n",
    "lstm_reg = hpo['lstm_regression']\n",
    "df_lstm_reg = pd.DataFrame(lstm_reg['all_results'])\n",
    "lp_reg = pd.json_normalize(df_lstm_reg['params'])\n",
    "df_lstm_reg = pd.concat([lp_reg, df_lstm_reg[['mean_score', 'std_score']]], axis=1)\n",
    "df_lstm_reg = df_lstm_reg.sort_values('mean_score')\n",
    "\n",
    "print(f\"LSTM Regression — Metric: {lstm_reg['metric']}  |  Best: {lstm_reg['best_score']:.4f}\")\n",
    "print(f\"Best params: {lstm_reg['best_params']}\\n\")\n",
    "df_lstm_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LSTM classification ──\n",
    "lstm_clf = hpo['lstm_classification']\n",
    "df_lstm_clf = pd.DataFrame(lstm_clf['all_results'])\n",
    "lp_clf = pd.json_normalize(df_lstm_clf['params'])\n",
    "df_lstm_clf = pd.concat([lp_clf, df_lstm_clf[['mean_score', 'std_score']]], axis=1)\n",
    "df_lstm_clf = df_lstm_clf.sort_values('mean_score')\n",
    "\n",
    "print(f\"LSTM Classification — Metric: {lstm_clf['metric']}  |  Best: {lstm_clf['best_score']:.4f}\")\n",
    "print(f\"Best params: {lstm_clf['best_params']}\\n\")\n",
    "df_lstm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71387e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, df_l, title, color in [\n",
    "    (axes[0], df_lstm_reg, 'LSTM Regression (MSE Loss)', '#10b981'),\n",
    "    (axes[1], df_lstm_clf, 'LSTM Classification (BCE Loss)', '#ef4444'),\n",
    "]:\n",
    "    scatter = ax.scatter(\n",
    "        df_l['learning_rate'], df_l['hidden_dim'],\n",
    "        s=200, c=df_l['mean_score'], cmap='viridis_r',\n",
    "        edgecolors='black', linewidths=0.5,\n",
    "    )\n",
    "    plt.colorbar(scatter, ax=ax, label='Mean CV Loss')\n",
    "    ax.set_xlabel('Learning Rate')\n",
    "    ax.set_ylabel('Hidden Dim')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGS / 'hpo_lstm_landscape.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8166fa4",
   "metadata": {},
   "source": [
    "## 5 — Best Parameters Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f66294",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for task_key in ['xgb_regression', 'xgb_classification', 'lstm_regression', 'lstm_classification']:\n",
    "    r = hpo[task_key]\n",
    "    model = 'XGBoost' if 'xgb' in task_key else 'LSTM'\n",
    "    summary_rows.append({\n",
    "        'Task': task_key,\n",
    "        'Model': model,\n",
    "        'Metric': r['metric'],\n",
    "        'Best Score': round(r['best_score'], 4),\n",
    "        'N Configs': r['n_configs'],\n",
    "        'N Splits': r['n_splits'],\n",
    "        'Best Params': str(r['best_params']),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "print(f\"Total HPO time: {hpo['elapsed_seconds']:.0f}s\\n\")\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best_params.json:')\n",
    "print(json.dumps(best, indent=2))\n",
    "print()\n",
    "print('Saved artifacts:')\n",
    "for p in [REPORTS / 'hpo_results.json', MODELS / 'best_params.json']:\n",
    "    print(f'  ✓ {p}  ({p.stat().st_size:,} bytes)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
