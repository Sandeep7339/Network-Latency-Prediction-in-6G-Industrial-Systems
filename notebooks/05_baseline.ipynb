{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6f4b6c",
   "metadata": {},
   "source": [
    "# 05 — Baseline Models\n",
    "\n",
    "Trains and evaluates baseline models for two tasks:\n",
    "\n",
    "| Task | Target | Models |\n",
    "|------|--------|--------|\n",
    "| Regression | `latency_us` | Mean predictor, Ridge, XGBoost |\n",
    "| Classification | `latency_violation` (>120 µs) | Logistic Regression, LightGBM |\n",
    "\n",
    "**Split:** time-ordered 70 / 15 / 15 on `timestamp_ns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e5994",
   "metadata": {},
   "source": [
    "## 1. Run training (or load saved results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(os.path.join(os.path.dirname(os.path.abspath('__file__')), '..'))\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from src.models.baseline import (\n",
    "    load_train_ready, derive_targets, time_split,\n",
    "    build_feature_transformer, _get_feature_names,\n",
    "    train_regression_models, train_classification_models,\n",
    "    evaluate_on_test, per_device_metrics_top10,\n",
    "    REG_TARGET, CLF_TARGET, VIOLATION_THRESHOLD_US,\n",
    ")\n",
    "\n",
    "# Load & split\n",
    "df = load_train_ready(\"data/train_ready.parquet\")\n",
    "df = derive_targets(df)\n",
    "train_df, val_df, test_df = time_split(df)\n",
    "\n",
    "# Build features (fit on train)\n",
    "ct = build_feature_transformer()\n",
    "X_train = ct.fit_transform(train_df)\n",
    "X_val   = ct.transform(val_df)\n",
    "X_test  = ct.transform(test_df)\n",
    "feature_names = _get_feature_names(ct)\n",
    "\n",
    "# Targets\n",
    "y_train_reg, y_val_reg, y_test_reg = train_df[REG_TARGET].values, val_df[REG_TARGET].values, test_df[REG_TARGET].values\n",
    "y_train_clf, y_val_clf, y_test_clf = train_df[CLF_TARGET].values, val_df[CLF_TARGET].values, test_df[CLF_TARGET].values\n",
    "\n",
    "print(f\"X_train: {X_train.shape}   Features: {len(feature_names)}\")\n",
    "print(f\"Violation rate — train:{y_train_clf.mean():.3f}  val:{y_val_clf.mean():.3f}  test:{y_test_clf.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954007a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "reg_results = train_regression_models(X_train, y_train_reg, X_val, y_val_reg)\n",
    "clf_results = train_classification_models(X_train, y_train_clf, X_val, y_val_clf)\n",
    "test_metrics = evaluate_on_test(reg_results, clf_results, X_test, y_test_reg, y_test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3b816",
   "metadata": {},
   "source": [
    "## 2. Metrics summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression summary\n",
    "reg_rows = []\n",
    "for name, res in reg_results.items():\n",
    "    row = {\"model\": name}\n",
    "    for split, m in [(\"val\", res[\"val_metrics\"]), (\"test\", test_metrics[name])]:\n",
    "        for k, v in m.items():\n",
    "            row[f\"{split}_{k}\"] = round(v, 4)\n",
    "    reg_rows.append(row)\n",
    "\n",
    "reg_df = pd.DataFrame(reg_rows).set_index(\"model\")\n",
    "print(\"\\nREGRESSION — latency_us\")\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary\n",
    "clf_rows = []\n",
    "for name, res in clf_results.items():\n",
    "    row = {\"model\": name}\n",
    "    for split, m in [(\"val\", res[\"val_metrics\"]), (\"test\", test_metrics[name])]:\n",
    "        for k, v in m.items():\n",
    "            row[f\"{split}_{k}\"] = round(v, 4)\n",
    "    clf_rows.append(row)\n",
    "\n",
    "clf_df = pd.DataFrame(clf_rows).set_index(\"model\")\n",
    "print(\"\\nCLASSIFICATION — latency_violation (>120 µs)\")\n",
    "clf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4697377",
   "metadata": {},
   "source": [
    "## 3. Predicted vs Actual — Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (name, res) in zip(axes, reg_results.items()):\n",
    "    y_pred = res[\"model\"].predict(X_test)\n",
    "    ax.scatter(y_test_reg, y_pred, alpha=0.15, s=8, edgecolors=\"none\")\n",
    "    lo, hi = y_test_reg.min(), y_test_reg.max()\n",
    "    ax.plot([lo, hi], [lo, hi], \"r--\", lw=1, label=\"ideal\")\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Actual latency_us\")\n",
    "    ax.set_ylabel(\"Predicted latency_us\")\n",
    "    m = test_metrics[name]\n",
    "    ax.text(0.05, 0.92, f\"R²={m['r2']:.4f}\\nMAE={m['mae']:.2f}\",\n",
    "            transform=ax.transAxes, fontsize=8, va=\"top\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n",
    "\n",
    "fig.suptitle(\"Predicted vs Actual — Regression baselines (test set)\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/baseline_pred_vs_actual.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ad8d5",
   "metadata": {},
   "source": [
    "## 4. Precision-Recall & ROC curves — Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df60d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for name, res in clf_results.items():\n",
    "    prob = res[\"model\"].predict_proba(X_test)[:, 1]\n",
    "    PrecisionRecallDisplay.from_predictions(\n",
    "        y_test_clf, prob, name=name, ax=ax1\n",
    "    )\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_test_clf, prob, name=name, ax=ax2\n",
    "    )\n",
    "\n",
    "ax1.set_title(\"Precision-Recall Curve\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax2.plot([0, 1], [0, 1], \"k--\", lw=0.8, label=\"random\")\n",
    "ax2.set_title(\"ROC Curve\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "\n",
    "fig.suptitle(\"Classification baselines — latency_violation (test set)\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/baseline_pr_roc.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d7f4b",
   "metadata": {},
   "source": [
    "## 5. Feature importance — XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a38b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = reg_results[\"xgboost_reg\"][\"model\"]\n",
    "importances = xgb_model.feature_importances_\n",
    "top_k = 20\n",
    "idx = np.argsort(importances)[-top_k:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.barh(np.array(feature_names)[idx], importances[idx])\n",
    "ax.set_xlabel(\"Importance (gain)\")\n",
    "ax.set_title(f\"Top-{top_k} features — XGBoost regression\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/baseline_xgb_importance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa70c59",
   "metadata": {},
   "source": [
    "## 6. Residual distribution — XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ec7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_test_reg - xgb_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.hist(resid, bins=60, edgecolor=\"white\", alpha=0.7)\n",
    "ax.axvline(0, color=\"red\", ls=\"--\")\n",
    "ax.set_xlabel(\"Residual (actual − predicted)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"XGBoost residuals (test)\\nmean={resid.mean():.2f}, std={resid.std():.2f}\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/baseline_xgb_residuals.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b5d35",
   "metadata": {},
   "source": [
    "## 7. Load saved metrics JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7370f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reports/baseline_metrics.json\") as f:\n",
    "    saved = json.load(f)\n",
    "\n",
    "print(\"Keys:\", list(saved.keys()))\n",
    "print(f\"\\nSplit: {saved['split']}\")\n",
    "print(f\"Features: {saved['n_features']}\")\n",
    "print(f\"Violation threshold: {saved['violation_threshold_us']} µs\")\n",
    "print(f\"Violation rate: {saved['violation_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df69f39",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "1. **All regression R² ≈ 0 (negative)** — the features have effectively zero\n",
    "   predictive power for `latency_us`. This is consistent with the EDA finding\n",
    "   that the synthetic data has near-uniform distributions and zero correlations.\n",
    "\n",
    "2. **Classification AUC ≈ 0.50** — both classifiers perform at random-guess\n",
    "   level. Again, expected given the data generating process.\n",
    "\n",
    "3. **Ridge ≈ Mean Predictor** — Ridge adds no lift, confirming no linear\n",
    "   relationship exists between features and `latency_us`.\n",
    "\n",
    "4. **XGBoost overfits slightly** (worse than mean on test) — with no real\n",
    "   signal, the tree ensemble captures noise.\n",
    "\n",
    "5. These baselines serve as a **reference floor**: any genuine improvement\n",
    "   from more sophisticated models or better-engineered features should\n",
    "   beat these numbers convincingly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
