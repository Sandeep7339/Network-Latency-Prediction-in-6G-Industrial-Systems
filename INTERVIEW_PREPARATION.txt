================================================================================
              TD-6GMF-INDSEC — INTERVIEW PREPARATION GUIDE
              (Time-Deterministic 6G Multi-Flow Industrial Security)
================================================================================

  Author  : Sandeep Soni, IIT Jodhpur
  Purpose : Quick-reference for interviews — every concept explained simply

================================================================================
                        TABLE OF CONTENTS
================================================================================

  1.  PROJECT OVERVIEW (What & Why)
  2.  THE DATASET — What Each File Contains
  3.  COLUMN-BY-COLUMN MEANING (Plain English)
  4.  DATA MERGING — How 6 Files Became 1
  5.  FEATURE ENGINEERING — What & Why
  6.  DATA SPLITTING — Why Time-Based
  7.  BASELINE MODELS
  8.  ADVANCED MODELS (XGBoost-ES & LSTM)
  9.  HYPERPARAMETER OPTIMISATION (HPO)
  10. STACKING ENSEMBLE
  11. EXPLAINABILITY (SHAP)
  12. ERROR ANALYSIS
  13. ROBUSTNESS TESTING
  14. ENFORCEMENT CAUSAL ANALYSIS (DiD & PSM)
  15. ONLINE PREDICTION PIPELINE
  16. KEY RESULTS TABLE
  17. WHY R²≈0 AND AUC≈0.5 — THE SYNTHETIC DATA EXPLANATION
  18. FOLDER STRUCTURE — WHAT EACH FILE DOES
  19. COMMON INTERVIEW QUESTIONS & ANSWERS


================================================================================
  1.  PROJECT OVERVIEW
================================================================================

What is this project?
---------------------
An end-to-end machine learning pipeline that analyses network traffic data
from a simulated 6G industrial environment.  It predicts:

  (a) How much delay (latency) each packet will have   → REGRESSION
  (b) Whether a packet will violate the SLA deadline    → CLASSIFICATION

It also answers:
  (c) Which features matter most?                      → SHAP
  (d) Is the model reliable under noise/attacks?        → ROBUSTNESS
  (e) Do enforcement actions actually reduce latency?   → CAUSAL ANALYSIS

Why 6G?
-------
6G networks aim for sub-millisecond latency in factories.  Machines (robots,
sensors, PLCs) need packets delivered within strict deadlines.  If a packet
arrives late, it is an "SLA violation."  This project builds the ML system
to predict and prevent such violations.

What is an SLA?
---------------
Service Level Agreement — a promise that latency will stay below a threshold.
Here, the threshold is 120 microseconds (μs).  Any packet with latency > 120 μs
is flagged as a "violation."


================================================================================
  2.  THE DATASET — WHAT EACH FILE CONTAINS
================================================================================

There are 6 raw CSV files.  Think of them as 6 different sensors/log sources
in a factory network:

  -----------------------------------------------------------------------
  File                         Rows      Cols   What it captures
  -----------------------------------------------------------------------
  Network_Traffic.csv          200,000    11    Every packet sent across
                                                the network: source,
                                                destination, size, latency,
                                                jitter, protocol used.

  Time_Deterministic_Stats.csv 200,000     7    Timing stats: cycle time,
                                                deadline, whether the
                                                deadline was met (1-to-1
                                                with Network_Traffic).

  Security_Events.csv           50,000     8    Detected attacks: type of
                                                attack, how severe, how
                                                unusual the traffic was.

  Enforcement_Actions.csv       50,000     7    What the system did in
                                                response to attacks:
                                                isolated traffic, rerouted,
                                                or applied access control.

  Stabilization_Controller.csv 200,000     6    Controller state: is the
                                                network normal, congested,
                                                or under attack? Queue
                                                sizes, rerouting counts.

  Device_Profile.csv             1,000    10    One row per device: CPU
                                                usage, memory, battery,
                                                trust score, vendor, type.
  -----------------------------------------------------------------------

After merging, we get 40,000 rows × 57 columns — one row per network packet
with all context attached.


================================================================================
  3.  COLUMN-BY-COLUMN MEANING
================================================================================

Below is every column in the merged dataset, in plain language.

  ─── From Network_Traffic ─────────────────────────────────────────────────

  timestamp_ns          When the packet was sent (nanoseconds since start).
  src_device_id         ID of the sending device (e.g., a sensor or PLC).
  dst_device_id         ID of the receiving device.
  traffic_type          What kind of data: "Sensor", "Control", "Video",
                        or "Command".
  packet_size_bytes     How big the packet is, in bytes.
  flow_priority         Priority level of this packet (higher = more urgent).
  scheduled_slot        The time-slot this packet was scheduled into (like
                        a reservation on a highway).
  protocol              Network protocol: "TSN", "DetNet", or "6G-URLLC".
                          TSN     = Time-Sensitive Networking (IEEE 802.1)
                          DetNet  = Deterministic Networking (IETF)
                          6G-URLLC= Ultra-Reliable Low Latency (3GPP)
  latency_us            *** TARGET (regression) ***
                        Actual one-way delay of this packet, in microseconds.
  jitter_us             Variation in delay — how unstable the latency is.
  packet_loss           Whether the packet was lost (0 = delivered, 1 = lost).

  ─── From Device_Profile ──────────────────────────────────────────────────

  device_type           "Sensor", "PLC", "Robot", or "Actuator".
  vendor                Manufacturer: "Siemens", "Bosch", "ABB", "Rockwell".
  firmware_version      Software version on the device: "v1.0", "v1.1", "v2.0".
  cpu_usage             How busy the device's processor is (0–100%).
  memory_usage          How much RAM is being used (0–100%).
  battery_level         Remaining battery (0–100%). Only for wireless devices.
  mobility_state        "Mobile" or "Static" — is the device moving?
  trust_score           A 0–1 score: how trustworthy the device is deemed.
  operational_state     "Normal", "Degraded", or "Fault".

  ─── From Security_Events ─────────────────────────────────────────────────

  event_id              Unique ID for each security event.
  attack_type           What kind of attack was detected:
                          DoS            = overwhelm with traffic
                          Spoofing       = fake the sender identity
                          MITM           = man-in-the-middle eavesdropping
                          Replay         = resend old packets
                          Data Injection = inject fake data
  traffic_deviation     How different this traffic is from normal (z-score).
  behavior_anomaly_score  Anomaly score from an intrusion detection system.
  anomaly_label         Always True (every row has a security event).
  severity_level        "Low", "Medium", "High", or "Critical".

  ─── From Enforcement_Actions ─────────────────────────────────────────────

  action_id             Unique ID for the enforcement action.
  action_type           What action was taken:
                          Access Control      = block suspicious traffic
                          Isolation           = disconnect the device
                          Traffic Redirection = reroute around the problem
  execution_slot        When the action was executed (slot number).
  enforcement_latency_us  How long the enforcement action itself took.
  success_flag          Did the action succeed? (1=yes, 0=no).
  affected_flows        How many network flows were impacted by this action.

  ─── From Stabilization_Controller ────────────────────────────────────────

  controller_state      "Normal", "Congested", or "Under Attack".
  queue_occupancy       How full the packet queues are (0–1). Higher = worse.
  rerouted_flows        Number of flows currently being rerouted.
  scheduling_adjustment How many scheduling changes the controller made.
  latency_restored      Binary: did the controller restore latency? (1=yes).
  control_action_delay_us  How long the controller's action took.

  ─── Engineered features (added by our pipeline) ─────────────────────────

  latency_roll_mean_1s   Average latency over the last 1 second.
  latency_roll_std_1s    Std-dev of latency over the last 1 second.
  packet_rate_1s         Number of packets in the last 1 second.
  latency_roll_mean_10s  Same, but over 10 seconds.
  latency_roll_std_10s   Same, but over 10 seconds.
  packet_rate_10s        Same, but over 10 seconds.
  latency_roll_mean_60s  Same, but over 60 seconds.
  latency_roll_std_60s   Same, but over 60 seconds.
  packet_rate_60s        Same, but over 60 seconds.
  latency_lag_1 … lag_10 Previous packet's latency (lag_1 = one step back,
                          lag_10 = ten steps back).

  ─── Derived target ──────────────────────────────────────────────────────

  latency_violation     1 if latency_us > 120 μs, else 0.
                        *** TARGET (classification) ***
                        About 9.6% of packets violate the SLA.


================================================================================
  4.  DATA MERGING — HOW 6 FILES BECAME 1
================================================================================

Step-by-step:

  1. Start with Network_Traffic (200k rows) — this is the "spine."
  2. LEFT JOIN Device_Profile on src_device_id.
     → Attaches device CPU, memory, vendor, etc. to each packet.
  3. MERGE_ASOF Security_Events on timestamp_ns.
     → For each packet, find the nearest security event (by time).
     merge_asof = "find the closest match that happened at or before."
  4. LEFT JOIN Enforcement_Actions on event_id.
     → Link the security event to the enforcement action taken.
  5. Row-aligned concatenation of Stabilization_Controller columns.
     → These have the same row order as Network_Traffic.
  6. Random sample to 40,000 rows for manageable experimentation.

  Result: 40,000 rows × 57 columns in data/train_ready.parquet.

  What is Parquet?
  ----------------
  A compressed columnar file format.  Much faster to read than CSV.
  Preserves data types (no need to re-parse strings).


================================================================================
  5.  FEATURE ENGINEERING — WHAT & WHY
================================================================================

Starting from 57 raw columns, we create 68 features for the model.

  ─── Numeric features (38 total) ──────────────────────────────────────────

  19 base numeric columns (packet_size, cpu_usage, jitter, queue_occupancy …)
    → Imputed with median (fill missing values with the middle value)
    → Standard-scaled (subtract mean, divide by std → mean=0, std=1)
    WHY:  Models like Ridge and LSTM are sensitive to feature scales.
          If one column ranges 0–1 and another 0–10000, the model
          would ignore the small one.  Scaling puts them on equal footing.

  9 rolling-window features (3 windows × 3 features):
    → latency_roll_mean, latency_roll_std, packet_rate
    → Windows: 1 second, 10 seconds, 60 seconds
    WHY:  Captures recent trends.  Is latency increasing?  Is traffic
          spiking?  A single row's value tells you "now", but rolling
          features tell you "what's been happening."

  10 lag features (latency_lag_1 through latency_lag_10):
    → Previous packets' latency values.
    WHY:  If the last few packets were slow, the next one might be too.
          Lag features let the model see recent history.

  ─── One-hot encoded features (22 total) ──────────────────────────────────

  7 categorical columns get one-hot encoding:
    traffic_type (4), protocol (3), device_type (4), vendor (4),
    mobility_state (2→1 after drop_if_binary), attack_type (5),
    action_type (3)

  What is one-hot encoding?
  -------------------------
  Turn a category like "Sensor" into a binary column:
    traffic_type_Sensor = 1, traffic_type_Video = 0, etc.
  WHY: ML models need numbers, not text.  One-hot avoids imposing a
       false order (Sensor < Video makes no sense).

  ─── Ordinal encoded features (3 total) ───────────────────────────────────

  3 columns with a natural order:
    operational_state:  Normal=0, Degraded=1, Fault=2
    severity_level:     Low=0, Medium=1, High=2, Critical=3
    controller_state:   Normal=0, Congested=1, Under Attack=2

  What is ordinal encoding?
  -------------------------
  Assign integers that respect the order.  "Critical" (3) is worse than
  "Low" (0).  Unlike one-hot, this preserves the ranking.

  ─── Frequency encoded features (3 total) ─────────────────────────────────

  3 high-cardinality columns (too many unique values for one-hot):
    src_device_id (1000 unique), dst_device_id (1000), firmware_version (3)

  What is frequency encoding?
  ---------------------------
  Replace each value with "how often it appears in the training set."
    Example: device_42 appears in 2% of rows → encoded as 0.02.
  WHY: One-hot with 1000 columns would be too sparse.  Frequency encoding
       compresses it into a single column that still carries information.

  ─── Summary ──────────────────────────────────────────────────────────────

   Type              Count   Example
   ─────────────────────────────────────────────────
   Numeric (scaled)    38    cpu_usage, latency_lag_1
   One-hot             22    traffic_type_Sensor
   Ordinal              3    severity_level
   Frequency            3    src_device_id_freq
   ─────────────────────────────────────────────────
   TOTAL               68    ← these go into models

  The whole transformation is wrapped in a scikit-learn ColumnTransformer,
  which means you call .fit_transform(train) and .transform(test) and it
  does everything automatically.  Saved as models/preprocess_pipeline.joblib.


================================================================================
  6.  DATA SPLITTING — WHY TIME-BASED
================================================================================

  Split:  70% train / 15% validation / 15% test
  Method: Chronological (sorted by timestamp_ns)

    Train:       rows 1–28,000    (timestamps ≤ 139,812,283 ns)
    Validation:  rows 28,001–34,000
    Test:        rows 34,001–40,000  (timestamps ≥ 169,900,350 ns)

  Why not random split?
  ---------------------
  In time-series data, future data should NEVER leak into training.
  If we randomly mix rows, the model might learn patterns from the
  future and look falsely accurate.  Time-split simulates the real
  deployment scenario: train on the past, predict the future.


================================================================================
  7.  BASELINE MODELS
================================================================================

Baselines are simple models that set a performance floor.  Any advanced
model must beat these or it is not worth the complexity.

  ─── Regression baselines ─────────────────────────────────────────────────

  1. DummyRegressor (Mean Predictor)
     What:  Always predicts the average latency of the training set.
     Why:   The simplest possible model.  If your fancy model can't
            beat "just guess the average," something is wrong.
     Result: MAE = 12.08 μs

  2. Ridge Regression
     What:  Linear regression + L2 regularisation (penalty on large weights).
     Why:   Fast, interpretable, works well when features are linearly
            related to the target.
     L2 regularisation:  adds λ × Σ(weight²) to the loss function.
            This prevents any single feature from getting a huge weight.
     Result: MAE = 12.10 μs

  3. XGBRegressor (200 rounds)
     What:  Gradient boosted decision trees.  Builds 200 small trees
            sequentially, each fixing the errors of the previous ones.
     Why:   Often the best "out-of-the-box" model for tabular data.
     Result: MAE = 12.11 μs

  ─── Classification baselines ─────────────────────────────────────────────

  4. Logistic Regression (balanced)
     What:  Linear model for binary outcomes + sigmoid function.
     "Balanced": weights rare class (violations) higher so the model
            doesn't just predict "no violation" for everything.
     Result: Accuracy = 38.4%, AUC = 0.505

  5. LGBMClassifier (200 rounds, balanced)
     What:  LightGBM — similar to XGBoost but uses histogram-based
            splitting (faster on large datasets).
     Result: Accuracy = 90.4%, AUC = 0.517


================================================================================
  8.  ADVANCED MODELS
================================================================================

  ─── XGBoost with Early Stopping (XGBoost-ES) ────────────────────────────

  What:  Same as baseline XGBoost, but with 500 rounds and "early stopping."

  Early stopping:  Stop adding trees when the validation score hasn't
    improved for 30 rounds.
    WHY:  Prevents overfitting.  Without it, XGBoost will keep adding
          trees that memorise the training data.

  Hyperparameters used:
    n_estimators = 500 (max trees)
    max_depth = 6
    learning_rate = 0.05
    subsample = 0.8 (use 80% of rows per tree → randomness → less overfit)
    colsample_bytree = 0.8 (use 80% of features per tree)
    reg_alpha = 0.1 (L1 regularisation — encourages sparse trees)
    reg_lambda = 1.0 (L2 regularisation — shrinks weights)

  Result:  MAE = 12.11 μs, AUC = 0.505

  ─── LSTM (Long Short-Term Memory) ────────────────────────────────────────

  What:  A type of Recurrent Neural Network (RNN) designed for sequences.

  Why LSTM over a regular RNN?
    Regular RNNs forget long sequences (vanishing gradient problem).
    LSTM has "gates" (forget gate, input gate, output gate) that
    decide what to remember and what to forget.

  How we use it:
    1. Sort all packets by timestamp.
    2. Create a sliding window of 30 consecutive packets.
    3. Feed the window through the LSTM → predict the 30th packet's latency.

  Architecture:
    LSTM layers:   2 stacked layers
    Hidden units:  64 per layer
    Dropout:       0.2 (randomly zero-out 20% of neurons during training
                   to prevent overfitting)
    Output:        Linear layer (regression) or Sigmoid layer (classification)

  Training details:
    Optimizer:     Adam (adaptive learning rate)
    Scheduler:     ReduceLROnPlateau — halve LR if val loss stalls for 3 epochs
    Early stop:    patience 8 epochs
    Batch size:    256
    Max epochs:    30
    Loss:          MSE (regression), BCE (classification)

  Result:  MAE = 12.08 μs, AUC = 0.494

  What is a sliding window?
  -------------------------
    Input: [packet_1, packet_2, …, packet_30]  → Predict: packet_30's latency
    Then:  [packet_2, packet_3, …, packet_31]  → Predict: packet_31's latency
    This gives the model "context" — what happened in the recent past.


================================================================================
  9.  HYPERPARAMETER OPTIMISATION (HPO)
================================================================================

  What is HPO?
  ------------
  Models have settings (hyperparameters) you choose before training.
  HPO = systematically try different settings and pick the best one.

  Method used: Random Search with Time-Series Cross-Validation.

  ─── XGBoost HPO ──────────────────────────────────────────────────────────

  Search space:
    max_depth:         [3, 6, 9]
    learning_rate:     [0.01, 0.05, 0.1]
    subsample:         [0.7, 0.9]
    colsample_bytree:  [0.7, 1.0]
    reg_alpha:         [0.0, 0.1]
    reg_lambda:        [1.0, 5.0]

  Configs tested:  12 random combinations
  CV folds:        4 (expanding-window time-series CV)
  Metric:          MAE (regression), log-loss (classification)

  Best regression params:
    max_depth=3, learning_rate=0.1, subsample=0.9,
    colsample_bytree=0.7, reg_alpha=0.0, reg_lambda=1.0
    Best CV MAE = 11.95

  Best classification params:
    max_depth=9, learning_rate=0.05, subsample=0.9,
    colsample_bytree=0.7, reg_alpha=0.1, reg_lambda=1.0
    Best CV log-loss = 0.326

  ─── LSTM HPO ─────────────────────────────────────────────────────────────

  Search space:
    hidden_dim:     [32, 64, 128]
    learning_rate:  [5e-4, 1e-3, 3e-3, 5e-3]
    dropout:        [0.1, 0.2, 0.3]

  Configs tested:  6 random combinations
  CV folds:        3 (expanding-window)
  Epochs per trial: 10 (short runs to save time)

  Best regression params:  hidden_dim=128, LR=0.005, dropout=0.1
  Best classification params: hidden_dim=128, LR=0.001, dropout=0.1

  ─── What is time-series cross-validation? ────────────────────────────────

  Regular k-fold CV shuffles data randomly.  That leaks future info.

  Time-series CV (expanding window):
    Fold 1: Train on [0–20%],    Validate on [20–40%]
    Fold 2: Train on [0–40%],    Validate on [40–60%]
    Fold 3: Train on [0–60%],    Validate on [60–80%]
    Fold 4: Train on [0–80%],    Validate on [80–100%]

  The training set always grows, and validation is always in the future.
  This mimics real life: you always have more past data over time.

  Total HPO time: ~44 minutes (2654 seconds).


================================================================================
  10. STACKING ENSEMBLE
================================================================================

  What is stacking?
  -----------------
  Combine multiple models into one stronger model.

  How it works (2 levels):

    Level 0 (base learners):  3 different models each make predictions.
    Level 1 (meta-learner):   A simple model takes those 3 predictions
                              as input and makes the final prediction.

  Regression stack:
    Base learners:    XGBoost, LightGBM, Ridge
    Meta-learner:     Ridge
    Each base learner predicts latency → 3 numbers → Ridge combines them.

  Classification stack:
    Base learners:    XGBoost, LightGBM, LogisticRegression
    Meta-learner:     LogisticRegression
    Each base learner predicts P(violation) → 3 probabilities → LR combines.

  Why stacking works:
    Different models have different strengths.  XGBoost might be good at
    catching non-linear patterns, Ridge at linear trends.  Stacking lets
    the meta-learner figure out "when to trust which model."

  Training process:
    1. Train base learners on the training set.
    2. Use them to predict on the validation set (out-of-fold predictions).
    3. Train the meta-learner on those validation predictions.
    4. For test evaluation: base learners predict → meta-learner combines.

  Result:
    Regression:     MAE = 12.09 μs (best individual: LightGBM at 12.08)
    Classification: Accuracy = 90.4%, AUC = 0.498

  Saved as: models/final_ensemble.joblib
    Contains: {reg: StackingEnsemble, clf: StackingEnsemble, transformer: CT}


================================================================================
  11. EXPLAINABILITY (SHAP)
================================================================================

  What is SHAP?
  -------------
  SHAP (SHapley Additive exPlanations) answers: "How much did each feature
  contribute to this particular prediction?"

  Based on Shapley values from game theory:
    Imagine each feature is a "player" in a team.  Shapley values calculate
    each player's fair contribution by considering all possible combinations
    of players.

  What we did:
    1. Took the XGBoost regression model.
    2. Used TreeExplainer (fast SHAP for tree models) on 2,000 test samples.
    3. Generated:
       - Global summary bar plot (which features matter most overall)
       - Dependence plots (how one feature's value affects the prediction)
       - Per-device SHAP plots (which features matter for specific devices)

  Top-5 most important features (by mean |SHAP|):
    1. packet_rate_60s          (0.023)
    2. vendor_Bosch             (0.020)
    3. attack_type_Data_Inj     (0.019)
    4. latency_lag_3            (0.018)
    5. firmware_version_freq    (0.017)

  Key insight:
    All SHAP values are very small (0.01–0.02 range).  This means NO
    feature strongly drives the prediction — expected because the synthetic
    data has no real signal.  On real data, you'd see some features with
    SHAP values 10–100× larger.

  How to read a SHAP summary plot:
    - Each dot = one data point.
    - X-axis = SHAP value (positive = increases prediction, negative = decreases).
    - Color = feature value (red = high, blue = low).
    - Features sorted top-to-bottom by importance.


================================================================================
  12. ERROR ANALYSIS
================================================================================

  What we checked:
    1. Confusion matrix — for the classifier, how many true positives,
       false positives, true negatives, false negatives.
    2. Error histogram — distribution of (predicted − actual) for regression.
    3. Worst-case traces — the 10 packets with the largest prediction errors.

  Findings:
    - Confusion matrix: model predicts "no violation" for almost everything
      (because 90.4% of test data is no-violation, and the model has no
      signal to distinguish).
    - Error histogram: symmetric around zero, bell-shaped — the model is
      not biased in any direction, just imprecise.
    - Worst errors: 30–50 μs off, occurring at extreme latency values
      (very low ~40 or very high ~160 μs, far from mean of 100).

  Figures generated:
    confusion_matrix.png
    error_histogram_regression.png
    error_histogram_classification.png


================================================================================
  13. ROBUSTNESS TESTING
================================================================================

  Three tests to check if the model breaks under stress:

  ─── Test 1: Slice Analysis ───────────────────────────────────────────────

  What:  Evaluate the model separately on subsets of the data.
  Slices: controller_state (Normal / Congested / Under Attack),
          severity_level (Low / Medium / High / Critical),
          attack_type (DoS / Spoofing / MITM / Replay / Data Injection).

  Result: MAE ≈ 12.1 μs across ALL slices.  No slice is significantly
          worse — the model is consistent.

  Why this matters:
    If the model works well overall but poorly on "Under Attack" data,
    that's dangerous.  Slice analysis catches such hidden failures.

  ─── Test 2: Noise Injection ─────────────────────────────────────────────

  What:  Add random noise to important features and see if the model breaks.
  Columns tested: queue_occupancy, packet_rate_1s
  Noise levels:
    - Clean (no noise)
    - Mild  (noise std = 0.5 × column std)
    - Heavy (noise std = 2.0 × column std)

  Result: MAE stays at 12.11 μs for all noise levels.
  Why:  The model already predicts near the mean, so noisy features
        don't change much.

  ─── Test 3: Concept Drift ───────────────────────────────────────────────

  What:  Simulate a scenario where the data distribution changes over time.
  Method: Train on the first 60% of data, test on the last 40%.
  Compare: vs. a model trained on all training data.

  Result: Both models give similar MAE (~12.0–12.6 μs).
  Why:  No real temporal shift exists in this synthetic data.
        On real data, drift detection would trigger retraining.

  Figures generated:
    robustness_slice_regression.png
    robustness_slice_classification.png
    robustness_noise_regression.png
    robustness_noise_classification.png
    robustness_drift.png


================================================================================
  14. ENFORCEMENT CAUSAL ANALYSIS
================================================================================

  The question:  "Do enforcement actions actually reduce latency?"
  This is a CAUSAL question, not just correlation.

  ─── Method 1: Pre/Post Window Analysis ──────────────────────────────────

  What:  For each enforcement event, look at packets in a ±100 μs window
         around the enforcement timestamp.

    Pre-window:   packets in [t − 100μs, t]  → "before" latency
    Post-window:  packets in [t, t + 100μs]  → "after" latency
    Delta = post_mean − pre_mean

  Result (per action type):
    ---------------------------------------------------------------
    Action Type            Events    Δ Mean     Δ P95
    ---------------------------------------------------------------
    Access Control           150    +0.30 μs   +0.66 μs
    Isolation                193    −0.00 μs   −0.05 μs
    Traffic Redirection      162    −0.22 μs   −0.25 μs
    ---------------------------------------------------------------

  Traffic Redirection shows the most latency reduction.

  ─── Method 2: Difference-in-Differences (DiD) ───────────────────────────

  What is DiD?
    A causal inference technique.  Compare the change in latency for:
      Treatment group:  packets near an enforcement event
      Control group:    packets far away (shifted 5× the window size)

    ATE = (Post_treatment − Pre_treatment) − (Post_control − Pre_control)

    If ATE < 0, the enforcement reduced latency beyond what would have
    happened naturally.

  Bootstrap:  Repeat statistical tests 2,000 times with resampling to
              get confidence intervals.

  Result:  All 95% CIs cross zero → NOT statistically significant.
           Expected for synthetic data with no true treatment effect.

  ─── Method 3: Propensity-Score Matching (PSM) ───────────────────────────

  What is PSM?
    Find "twins" — control packets that look similar to treated packets
    on all observable features, but didn't receive enforcement.

  Steps:
    1. Fit a logistic regression: P(treated | features) = propensity score.
    2. For each treated packet, find the nearest control packet by
       propensity score (using nearest-neighbor matching).
    3. Compare latency: treated vs. matched control.
    4. ATE = mean(latency_treated − latency_matched_control).

  Covariates used for matching (9):
    cpu_usage, memory_usage, queue_occupancy, trust_score,
    packet_size_bytes, flow_priority, traffic_deviation,
    behavior_anomaly_score, queue_delay_us

  Result:  ATE ≈ −7 to −9 μs with p < 0.001.
           BUT: this is likely an artifact of the matching procedure
           on synthetic data, not a real causal effect.

  Why PSM ≠ true causation here:
    PSM only controls for observed confounders.  With synthetic uniform
    data, the matched pairs can have systematic differences in
    unobserved factors.

  Figures generated:
    enforcement_pre_post.png
    enforcement_ate_forest.png
    enforcement_effectiveness_heatmap.png


================================================================================
  15. ONLINE PREDICTION PIPELINE
================================================================================

  What:  A command-line tool that takes new packet data (in JSON format)
         and outputs latency predictions + violation probabilities.

  Usage:
    python -m src.predict.online_predict \
        --input examples/sample_input.json \
        --output examples/sample_output.json

  Input format:
    {
      "flows": [
        {
          "timestamp_ns": 100000000,
          "src_device_id": 42,
          "traffic_type": "Sensor",
          "packet_size_bytes": 256,
          ...all 57 columns...
        }
      ]
    }

  Output format:
    {
      "predictions": [
        {
          "flow_index": 0,
          "predicted_latency_us": 99.12,
          "violation_probability": 0.07,
          "violation_flag": false
        }
      ]
    }

  How it works internally:
    1. Load models/final_ensemble.joblib (contains transformer + ensemble).
    2. Read JSON → convert to DataFrame.
    3. Transform with saved ColumnTransformer (same preprocessing as training).
    4. Predict with regression ensemble → latency.
    5. Predict with classification ensemble → violation probability.
    6. If probability ≥ 0.5, flag as violation.
    7. Write output JSON.


================================================================================
  16. KEY RESULTS TABLE
================================================================================

  ─── Regression (Test Set, 6,000 rows) ────────────────────────────────────

  ┌──────────────────┬──────────┬──────────┬────────────┐
  │ Model            │ MAE (μs) │ RMSE(μs) │ R²         │
  ├──────────────────┼──────────┼──────────┼────────────┤
  │ DummyRegressor   │  12.08   │  15.12   │ −0.0001    │
  │ Ridge            │  12.10   │  15.15   │ −0.0035    │
  │ XGBoost-ES       │  12.11   │  15.15   │ −0.0039    │
  │ LightGBM         │  12.08   │  15.13   │ −0.0006    │
  │ LSTM             │  12.08   │  15.12   │ −0.0007    │
  │ Ensemble         │  12.09   │  15.14   │ −0.0018    │
  └──────────────────┴──────────┴──────────┴────────────┘

  ─── Classification (Test Set) ────────────────────────────────────────────

  ┌──────────────────┬──────────┬──────┬──────────┐
  │ Model            │ Accuracy │ F1   │ AUC-ROC  │
  ├──────────────────┼──────────┼──────┼──────────┤
  │ LogisticReg      │  0.384   │ 0.17 │  0.505   │
  │ XGBoost-ES       │  0.893   │ 0.04 │  0.505   │
  │ LightGBM         │  0.904   │ 0.00 │  0.517   │
  │ LSTM             │  0.905   │ 0.00 │  0.494   │
  │ Ensemble         │  0.904   │ 0.00 │  0.498   │
  └──────────────────┴──────────┴──────┴──────────┘


================================================================================
  17. WHY R² ≈ 0 AND AUC ≈ 0.5
================================================================================

  This is the MOST IMPORTANT thing to understand for interviews.

  The data is SYNTHETIC (computer-generated).

  The latency values were drawn from a uniform distribution (~U(40, 160))
  INDEPENDENTLY of the features.  That means:
    - cpu_usage, attack_type, queue_occupancy, etc. have NO influence
      on latency.
    - No model can do better than "predict the mean" because there is
      no pattern to learn.

  What does R² ≈ 0 mean?
  -----------------------
    R² = 1 − (sum of squared errors / total variance)
    R² = 0 means the model explains 0% of the variance.
    R² < 0 means the model is WORSE than just predicting the mean.
    Our models get R² ≈ −0.003, basically 0.

  What does AUC ≈ 0.5 mean?
  --------------------------
    AUC = 0.5 means the model is no better than random guessing.
    AUC = 1.0 would be perfect classification.
    Our ~0.50 confirms there's no separability between violation/non-violation.

  Why is this OK?
  ---------------
    1. The PIPELINE is correct.  Every step works: data loading, feature
       engineering, model training, evaluation, deployment.
    2. If you plug in real data (where features actually predict latency),
       the same pipeline will produce meaningful R² and AUC.
    3. We verified this by checking that DummyRegressor ≈ XGBoost ≈ LSTM.
       If all models give the same result, the data is the bottleneck,
       not the models.

  How to explain in an interview:
    "The dataset is synthetic and uniformly distributed, so there is no
     learnable signal.  All models correctly converge to predicting the
     mean.  This validates the pipeline's correctness.  On real 6G
     testbed data, we expect significant improvements."


================================================================================
  18. FOLDER STRUCTURE — WHAT EACH FILE DOES
================================================================================

  CN_project/
  │
  ├── data/
  │   ├── Combined_Dataset.csv         Original merged CSV (large)
  │   ├── Device_Profile.csv           1,000 device records
  │   ├── Network_Traffic.csv          200,000 packet records
  │   ├── Time_Deterministic_Stats.csv 200,000 timing stats
  │   ├── Security_Events.csv          50,000 attack records
  │   ├── Enforcement_Actions.csv      50,000 action records
  │   ├── Stabilization_Controller.csv 200,000 controller states
  │   ├── network_latency_estimating_data.csv  (extra reference)
  │   └── train_ready.parquet          40,000×57 merged dataset (used by all)
  │
  ├── scripts/
  │   └── make_combined_40k.py         Merges 6 CSVs → train_ready.parquet
  │
  ├── src/
  │   ├── data/
  │   │   └── load_data.py             Helper to load any CSV/parquet
  │   │
  │   ├── features/
  │   │   └── feature_pipeline.py      FrequencyEncoder, RollingLagFeatures,
  │   │                                ColumnTransformer (57 cols → 68 features)
  │   │
  │   ├── models/
  │   │   ├── baseline.py              Dummy, Ridge, XGB, LR, LightGBM baselines
  │   │   ├── advanced.py              XGBoost-ES + LSTM (PyTorch)
  │   │   ├── hpo.py                   Hyperparameter search (XGB + LSTM)
  │   │   ├── ensemble.py              Stacking ensemble (final model)
  │   │   ├── compare.py               Model comparison utilities
  │   │   └── enforcement_effects.py   Causal analysis: DiD, PSM, pre/post
  │   │
  │   ├── eval/
  │   │   ├── explain.py               SHAP explainability
  │   │   ├── error_analysis.py        Confusion matrix, error histograms
  │   │   └── robustness.py            Slice, noise, drift testing
  │   │
  │   └── predict/
  │       └── online_predict.py        CLI tool: JSON in → predictions out
  │
  ├── models/                          Saved model files
  │   ├── preprocess_pipeline.joblib   ColumnTransformer
  │   ├── baseline_transformer.joblib  Same transformer (baseline copy)
  │   ├── baseline_mean_predictor.joblib
  │   ├── baseline_ridge.joblib
  │   ├── baseline_xgboost_reg.joblib
  │   ├── baseline_logistic.joblib
  │   ├── baseline_lightgbm_clf.joblib
  │   ├── advanced_xgb.joblib          XGBoost-ES reg+clf + eval curves
  │   ├── advanced_lstm.pt             LSTM weights (PyTorch)
  │   ├── advanced_transformer.joblib
  │   └── final_ensemble.joblib        Stacking ensemble + transformer
  │
  ├── figures/                         36 generated plots
  │   ├── 01_missingness_heatmap.png … 12_pair_plot.png   (EDA)
  │   ├── shap_summary.png, shap_dependence_*.png         (SHAP)
  │   ├── confusion_matrix.png, error_histogram_*.png     (Errors)
  │   ├── robustness_slice_*.png, robustness_noise_*.png  (Robustness)
  │   ├── robustness_drift.png
  │   ├── enforcement_pre_post.png                        (Causal)
  │   ├── enforcement_ate_forest.png
  │   ├── enforcement_effectiveness_heatmap.png
  │   ├── model_comparison.png
  │   └── final_metrics.png
  │
  ├── reports/
  │   ├── baseline_metrics.json        All baseline model metrics
  │   ├── advanced_metrics.json        XGBoost-ES + LSTM metrics
  │   ├── hpo_results.json             HPO search results + best params
  │   ├── final_metrics.json           Ensemble + individual test metrics
  │   ├── eda_summary.md               EDA observations
  │   ├── advanced_comparison.md       Baseline vs advanced comparison
  │   ├── explainability.md            SHAP findings + top features
  │   ├── robustness.md                Slice / noise / drift results
  │   ├── enforcement_analysis.md      Causal analysis + recommendations
  │   └── enforcement_effects_summary.csv  Raw causal estimates
  │
  ├── notebooks/                       Step-by-step Jupyter notebooks
  │   ├── 00_quick_start.ipynb         Environment verification
  │   ├── 01_data_ingest.ipynb         Data loading demo
  │   ├── 02_EDA.ipynb                 Exploratory data analysis
  │   ├── 04_features.ipynb            Feature engineering walkthrough
  │   ├── 05_baseline.ipynb            Baseline training
  │   ├── 06_advanced.ipynb            XGBoost-ES + LSTM training
  │   ├── 07_hpo.ipynb                 Hyperparameter search
  │   ├── 08_explainability.ipynb      SHAP analysis
  │   ├── 09_robustness.ipynb          Robustness evaluation
  │   ├── 10_enforcement_analysis.ipynb Causal analysis
  │   └── 11_predict_demo.ipynb        Online prediction demo
  │
  ├── examples/
  │   ├── sample_input.json            3 sample flows for prediction
  │   └── sample_output.json           Predicted latency + violation flags
  │
  ├── tests/                           50 passing tests
  │   ├── test_skeleton.py             (6)  Project structure checks
  │   ├── test_data_ingest.py          (13) Data loading & merging
  │   ├── test_features.py             (14) Feature engineering pipeline
  │   └── test_baseline.py             (17) Model training & metrics
  │
  ├── final_report.tex                 LaTeX report (Overleaf-ready)
  ├── figures_needed.txt               Figure mapping for LaTeX
  ├── requirements.txt                 Python dependencies
  ├── run_env_check.sh                 One-command setup script
  └── README.md                        Project overview


================================================================================
  19. COMMON INTERVIEW QUESTIONS & ANSWERS
================================================================================

  Q: What is the project about?
  A: Predicting packet latency and SLA violations in a simulated 6G
     industrial network using ML, with explainability and causal analysis.

  ........................................................................

  Q: Why did you choose XGBoost and LightGBM?
  A: They are the best general-purpose models for tabular data.  They
     handle non-linearities, missing values, and feature interactions
     automatically.  XGBoost builds trees sequentially (each correcting
     the previous), while LightGBM uses histogram-based splitting for
     speed.

  ........................................................................

  Q: Why an LSTM?
  A: Network traffic is sequential — packet latencies are correlated
     over time.  LSTM captures temporal dependencies that tree models
     ignore.  We used a sliding window of 30 packets as input.

  ........................................................................

  Q: Why stacking instead of just using the best model?
  A: Different models capture different patterns.  Stacking lets a
     meta-learner automatically learn which model to trust in which
     situation.  It often gives 1–5% improvement.

  ........................................................................

  Q: What is the difference between MAE, RMSE, and R²?
  A: MAE = average |error|.  Easy to interpret ("off by 12 μs on average").
     RMSE = sqrt(average error²).  Penalises big errors more than MAE.
     R² = 1 − (model error / baseline error).  0 = as good as the mean,
     1 = perfect, negative = worse than the mean.

  ........................................................................

  Q: Why is your R² negative?
  A: The synthetic data has no learnable signal (latency is random,
     independent of features).  So every model ≈ mean predictor.
     Slight negative R² means some models are fractionally worse than
     the mean due to noise.  This validates correctness — no model
     is overfitting fake patterns.

  ........................................................................

  Q: How do you prevent data leakage?
  A: Time-based split (train on past, test on future).  Feature pipeline
     is fit only on training data, then transform is applied to val/test.
     Rolling/lag features are computed before splitting, but only use
     past values (shift, rolling with min_periods=1).

  ........................................................................

  Q: What is frequency encoding and when would you use it?
  A: Replace a category with its training-set proportion.
     Use it for high-cardinality features (like device IDs with 1000
     unique values) where one-hot would create too many sparse columns.

  ........................................................................

  Q: What is SHAP and why use it?
  A: SHAP gives each feature a signed contribution to each prediction.
     Based on game-theory Shapley values.  We use TreeExplainer (fast
     for XGBoost).  It answers "why did the model predict 105 μs for
     this packet?" — e.g., "queue_occupancy contributed +3 μs."

  ........................................................................

  Q: What is Difference-in-Differences (DiD)?
  A: A causal inference method.  Compare the change in treated group vs.
     the change in a control group.  If enforcement truly helps, treated
     latency should drop MORE than control latency.  ATE = (ΔT − ΔC).
     We use bootstrap (resample 2000 times) for confidence intervals.

  ........................................................................

  Q: What is Propensity Score Matching (PSM)?
  A: Find untreated packets that "look like" treated packets on all
     features.  Then compare their latencies.  The propensity score =
     P(treated | features), estimated by logistic regression.  Match
     by nearest-neighbor on this single number.

  ........................................................................

  Q: Why use both DiD and PSM?
  A: They have complementary assumptions.  DiD assumes parallel trends
     (control and treatment would move similarly without intervention).
     PSM assumes no unobserved confounders (all relevant variables are
     measured).  Using both gives more robust evidence.

  ........................................................................

  Q: How would this change with real data?
  A: R² would be > 0 (features would actually predict latency).
     AUC > 0.8 (violations would be identifiable).
     SHAP would reveal which features truly matter.
     DiD would show if enforcement actually lowered latency.
     The entire pipeline remains the same — just plug in real data.

  ........................................................................

  Q: What are the 50 tests checking?
  A: test_skeleton (6): folder structure, requirements.txt, imports.
     test_data_ingest (13): CSV loading, parquet creation, column checks.
     test_features (14): pipeline shapes, NaN-free output, encoder logic.
     test_baseline (17): model training, metric calculations, artifact saving.

  ........................................................................

  Q: How would you deploy this in production?
  A: 1. Serve online_predict.py behind a REST API (Flask/FastAPI).
     2. Stream packets through it in real time.
     3. If violation_probability > 0.5, trigger enforcement action.
     4. Monitor concept drift monthly; retrain if needed.
     5. Use Grafana dashboard for latency alerts.

  ........................................................................

  Q: What regularisation techniques did you use?
  A: Ridge: L2 (penalty on weight magnitudes → prevents large weights).
     XGBoost: L1 (reg_alpha, sparse) + L2 (reg_lambda, smooth) on
     leaf weights, plus max_depth limit and subsampling.
     LSTM: Dropout (randomly zero-out neurons) + gradient clipping
     (prevent exploding gradients) + early stopping.

  ........................................................................

  Q: What is early stopping?
  A: Monitor validation loss during training.  If it doesn't improve
     for N epochs/rounds (patience), stop training.  Prevents overfitting
     — the model stops before it starts memorising training noise.

  ........................................................................

  Q: Why 68 features and not more/fewer?
  A: We combined domain knowledge with practical constraints:
     - 19 base numeric columns cover all available measurements.
     - 9 rolling features capture short/medium/long-term trends.
     - 10 lags capture recent history.
     - 22 one-hot columns encode all meaningful categories.
     - 3 ordinal columns preserve natural ordering.
     - 3 frequency encodings handle high-cardinality IDs.
     Total: 68 = enough to capture diverse aspects without overfitting.


================================================================================
  END OF PREPARATION GUIDE
================================================================================
  Full pipeline: 50 tests passing | 12 phases | 36 figures | 10 reports
  Tools: Python 3.11, scikit-learn, XGBoost, LightGBM, PyTorch, SHAP
  Random seed: 42 everywhere for reproducibility
================================================================================
